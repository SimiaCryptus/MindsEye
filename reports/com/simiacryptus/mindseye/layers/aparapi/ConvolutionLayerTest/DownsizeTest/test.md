# ConvolutionLayer
## DownsizeTest
### Json Serialization
Code from [LayerTestBase.java:84](../../../../../../../../../MindsEye/src/test/java/com/simiacryptus/mindseye/layers/LayerTestBase.java#L84) executed in 0.00 seconds: 
```java
    JsonObject json = layer.getJson();
    NNLayer echo = NNLayer.fromJson(json);
    assert (echo != null) : "Failed to deserialize";
    assert (layer != echo) : "Serialization did not copy";
    Assert.assertEquals("Serialization not equal", layer, echo);
    return new GsonBuilder().setPrettyPrinting().create().toJson(json);
```

Returns: 

```
    {
      "class": "com.simiacryptus.mindseye.layers.aparapi.ConvolutionLayer",
      "id": "e2d0bffa-47dc-4875-864f-3d3d00000014",
      "isFrozen": false,
      "name": "ConvolutionLayer/e2d0bffa-47dc-4875-864f-3d3d00000014",
      "filter": {
        "dimensions": [
          3,
          3,
          21
        ],
        "data": [
          -0.592,
          -1.712,
          1.78,
          0.556,
          1.296,
          -1.84,
          1.84,
          0.192,
          1.552,
          1.972,
          -0.152,
          0.416,
          0.964,
          1.996,
          -0.056,
          1.228,
          1.812,
          -1.596,
          0.056,
          1.292,
          1.08,
          -1.176,
          1.812,
          0.02,
          1.12,
          -1.56,
          0.24,
          1.0,
          0.628,
          -1.472,
          0.108,
          0.06,
          0.32,
          -1.816,
          1.608,
          -1.604,
          -0.16,
          0.34,
          -0.58,
          0.296,
          -1.404,
          1.172,
          -0.048,
          0.06,
          -1.236,
          1.0,
          -0.236,
          -0.38,
          0.5,
          1.816,
          -1.2,
          1.576,
          1.512,
          -1.076,
          0.272,
          1.8,
          0.252,
          -1.468,
          -0.356,
          1.312,
          0.888,
          -0.92,
          0.844,
          1.832,
          1.952,
          0.064,
          -1.104,
          1.76,
          0.384,
          -1.608,
          -1.36,
          -0.632,
          -1.496,
          1.844,
          -0.988,
          1.088,
          1.212,
          -1.408,
          1.484,
          1.796,
          -0.436,
          1.552,
          1.78,
          -1.88,
          -1.836,
          1.184,
          -0.492,
          1.46,
          -0.056,
          0.864,
          -1.024,
          0.836,
          -1.516,
          -1.992,
          0.752,
          1.36,
          -1.568,
          -0.064,
          -0.296,
          -0.344,
          0.424,
          0.4,
          1.388,
          1.444,
          0.644,
          -0.152,
          -0.908,
          1.928,
          -1.26,
          -1.728,
          1.984,
          -0.748,
          -0.212,
          0.244,
          -1.768,
          -0.4,
          -1.076,
          -0.704,
          -0.748,
          -0.56,
          -0.936,
          1.464,
          0.004,
          -1.364,
          -1.62,
          -1.036,
          -1.984,
          0.072,
          1.948,
          1.228,
          0.424,
          -1.496,
          -0.488,
          -1.384,
          -1.06,
          -1.068,
          -1.024,
          0.948,
          -0.7,
          0.984,
          1.152,
          -0.708,
          0.144,
          0.928,
          1.72,
          -1.868,
          1.928,
          1.14,
          1.988,
          0.716,
          -1.056,
          1.324,
          1.972,
          -1.7,
          -0.328,
          -0.524,
          1.728,
          -1.772,
          0.6,
          -1.024,
          1.292,
          -0.54,
          0.036,
          1.668,
          -1.268,
          0.84,
          -0.776,
          0.364,
          1.984,
          -0.364,
          -0.704,
          0.664,
          0.436,
          -1.648,
          -0.064,
          0.216,
          -0.04,
          1.264,
          -0.172,
          -1.364,
          1.512,
          1.76,
          1.076,
          1.924,
          -0.768,
          -0.376,
          0.244,
          0.232,
          -0.452
        ]
      },
      "skip": {
        "dimensions": [
          1,
          1
        ]
      },
      "simple": false
    }
```



### Example Input/Output Pair
Code from [LayerTestBase.java:121](../../../../../../../../../MindsEye/src/test/java/com/simiacryptus/mindseye/layers/LayerTestBase.java#L121) executed in 0.01 seconds: 
```java
    SimpleEval eval = SimpleEval.run(layer, inputPrototype);
    return String.format("--------------------\nInput: \n[%s]\n--------------------\nOutput: \n%s",
      Arrays.stream(inputPrototype).map(t->t.prettyPrint()).reduce((a,b)->a+",\n"+b).get(),
      eval.getOutput().prettyPrint());
```

Returns: 

```
    --------------------
    Input: 
    [[
    	[ [ -1.76, -0.468, 1.228, 1.192, -1.408, 1.068, 1.92 ], [ -0.912, -1.016, -1.98, -0.128, -0.62, -1.144, 1.892 ], [ 0.992, 1.436, -1.716, -0.572, 1.74, -1.908, -2.0 ] ],
    	[ [ -1.212, 1.472, 0.78, -1.716, -1.784, -0.104, 1.572 ], [ -0.04, 1.544, 1.232, 1.18, 0.948, 0.668, -1.112 ], [ -0.684, -0.808, 1.748, -0.748, -1.324, 0.136, -1.004 ] ],
    	[ [ 0.236, 0.668, -0.544, -1.248, -0.388, 0.844, 1.284 ], [ -1.28, -1.648, 0.992, -1.284, 0.748, 1.988, -0.992 ], [ 1.696, 1.152, -0.972, 0.088, -1.664, -0.676, 0.832 ] ]
    ]]
    --------------------
    Output: 
    [
    	[ [ 3.4604959999999996, 1.73632, 1.067216 ] ]
    ]
```



### Differential Validation
Code from [LayerTestBase.java:139](../../../../../../../../../MindsEye/src/test/java/com/simiacryptus/mindseye/layers/LayerTestBase.java#L139) executed in 1.18 seconds: 
```java
    getDerivativeTester().test(layer, inputPrototype);
```
Logging: 
```
    Feedback for input 0
    Inputs: [
    	[ [ -1.76, -0.468, 1.228, 1.192, -1.408, 1.068, 1.92 ], [ -0.912, -1.016, -1.98, -0.128, -0.62, -1.144, 1.892 ], [ 0.992, 1.436, -1.716, -0.572, 1.74, -1.908, -2.0 ] ],
    	[ [ -1.212, 1.472, 0.78, -1.716, -1.784, -0.104, 1.572 ], [ -0.04, 1.544, 1.232, 1.18, 0.948, 0.668, -1.112 ], [ -0.684, -0.808, 1.748, -0.748, -1.324, 0.136, -1.004 ] ],
    	[ [ 0.236, 0.668, -0.544, -1.248, -0.388, 0.844, 1.284 ], [ -1.28, -1.648, 0.992, -1.284, 0.748, 1.988, -0.992 ], [ 1.696, 1.152, -0.972, 0.088, -1.664, -0.676, 0.832 ] ]
    ]
    Output: [
    	[ [ 3.4604959999999996, 1.73632, 1.067216 ] ]
    ]
    Measured: [ [ -0.5919999999992598, 1.972000000001195, 0.05600000000161032 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.9999999999976694, -0.16000000000016001, 0.9999999999976694 ] ]
    Implemented: [ [ -0.592, 1.972, 0.056 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 1.0, -0.16, 1.0 ] ]
    Error: [ [ 7.401856905175919E-13, 1.1950440637065185E-12, 1.6103160471736544E-12 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ -2.3305801732931286E-12, -1.6001089342410069E-13, -2.3305801732931286E-12 ] ]
    Learning Gradient for weight set 0
    Inputs: [
    	[ [ -1.76, -0.468, 1.228, 1.192, -1.408, 1.068, 1.92 ], [ -0.912, -1.016, -1.98, -0.128, -0.62, -1.144, 1.892 ], [ 0.992, 1.436, -1.716, -0.572, 1.74, -1.908, -2.0 ] ],
    	[ [ -1.212, 1.472, 0.78, -1.716, -1.784, -0.104, 1.572 ], [ -0.04, 1.544, 1.232, 1.18, 0.948, 0.668, -1.112 ], [ -0.684, -0.808, 1.748, -0.748, -1.324, 0.136, -1.004 ] ],
    	[ [ 0.236, 0.668, -0.544, -1.248, -0.388, 0.844, 1.284 ], [ -1.28, -1.648, 0.992, -1.284, 0.748, 1.988, -0.992 ], [ 1.696, 1.152, -0.972, 0.088, -1.664, -0.676, 0.832 ] ]
    ]
    Outputs: [
    	[ [ 3.4604959999999996, 1.73632, 1.067216 ] ]
    ]
    Measured Gradient: [ [ -1.7599999999973193, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, -1.7599999999973193, 0.0 ] ]
    Implemented Gradient: [ [ -1.76, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, -1.76, 0.0 ] ]
    Error: [ [ 2.680744515259903E-12, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 0.0, 0.0 ], [ 0.0, 2.680744515259903E-12, 0.0 ] ]
    Finite-Difference Derivative Accuracy:
    absoluteTol: 8.9358e-14 +- 4.6216e-13 [0.0000e+00 - 5.6912e-12] (756#)
    relativeTol: 2.1821e-12 +- 7.0637e-12 [1.0253e-14 - 4.5217e-11] (42#)
    
```

### Performance
Code from [LayerTestBase.java:144](../../../../../../../../../MindsEye/src/test/java/com/simiacryptus/mindseye/layers/LayerTestBase.java#L144) executed in 0.81 seconds: 
```java
    getPerformanceTester().test(layer, inputPrototype);
```
Logging: 
```
    Evaluation performance: 30.1572 +- 3.3219 [25.3460 - 39.1561]
    Learning performance: 21.1482 +- 1.2826 [18.9197 - 25.5199]
    
```

